{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbd32ee2196f8464",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TripletGestureDataset(Dataset):\n",
    "    fingers = [\n",
    "        [0, 1, 2, 3, 4],\n",
    "        [0, 5, 6, 7, 8],\n",
    "        [0, 9, 10, 11, 12],\n",
    "        [0, 13, 14, 15, 16],\n",
    "        [0, 17, 18, 19, 20]\n",
    "    ]\n",
    "\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray):\n",
    "        \"\"\"\n",
    "        初始化函数。\n",
    "        :param data: 形状为(n, 21, 3)的numpy数组，n为样本数量。\n",
    "        :param labels: 形状为(n, num_classes)的numpy数组，每个标签为one-hot编码。\n",
    "        \"\"\"\n",
    "\n",
    "        # ## 数据增强\n",
    "        # data = np.abs(data)\n",
    "        # data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "        #将所有数据减去第一个关节点的坐标，并删除第一个关节点的坐标\n",
    "        data = data - data[:, 0:1, :]\n",
    "        data = np.delete(data, 0, axis=1)\n",
    "\n",
    "        ## add the two nearby length in each finger\n",
    "        for finger in self.fingers:\n",
    "            for i in range(len(finger) - 1):\n",
    "                dist = data[:, finger[i + 1]] - data[:, finger[i]]\n",
    "                # add a new dimension\n",
    "                dist = np.expand_dims(dist, axis=1)\n",
    "                data = np.concatenate((data, dist), axis=1)\n",
    "\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        # 将one-hot编码的标签转换为类别索引\n",
    "        self.labels = np.argmax(labels, axis=1)\n",
    "        # 预计算每个类别的索引列表，以便快速随机选择样本\n",
    "        self.indices = [np.where(self.labels == i)[0] for i in np.unique(self.labels)]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集中的样本数。\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        从数据集中获取一个样本及其正例和负例。\n",
    "        :param idx: 锚点样本的索引。\n",
    "        \"\"\"\n",
    "        anchor = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 选择正例，即同一类别中的另一个样本\n",
    "        positive_index = idx\n",
    "        while positive_index == idx:  # 确保正例不是锚点本身\n",
    "            positive_index = np.random.choice(self.indices[label])\n",
    "        positive = self.data[positive_index]\n",
    "\n",
    "        # 选择负例，即不同类别的样本\n",
    "        negative_label = np.random.choice([l for l in range(len(self.indices)) if l != label])\n",
    "        negative_index = np.random.choice(self.indices[negative_label])\n",
    "        negative = self.data[negative_index]\n",
    "\n",
    "        return anchor, positive, negative, label\n",
    "\n",
    "    def get_input_dim(self):\n",
    "        return self.data.shape[1] * self.data.shape[2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d862753b4f984735",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## load the data\n",
    "raw_data = np.load('./dataset/8class_dataset_100k.npz')\n",
    "train_data, train_label, test_data, test_label = raw_data['train_data'], raw_data['train_label'], raw_data['test_data'], \\\n",
    "    raw_data['test_label']\n",
    "\n",
    "train_dataset = TripletGestureDataset(train_data, train_label)\n",
    "test_dataset = TripletGestureDataset(test_data, test_label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "data_dim = train_dataset.get_input_dim()\n",
    "print(data_dim)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea496ccd7d2a1a1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "latent_dim = 4\n",
    "\n",
    "\n",
    "# 初始化模型, 损失函数, 和优化器\n",
    "class TripletAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=2):\n",
    "        super(TripletAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TripletAutoencoder(input_dim=data_dim, latent_dim=latent_dim).to(device)\n",
    "\n",
    "reconstruction_loss = nn.MSELoss()\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1.0)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d84fea01cd07c315",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## summary the model\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (data_dim,), 1024, \"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa7ccc32d6b2ce93",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    total_loss = 0\n",
    "    total_r_loss = 0\n",
    "    total_t_loss = 0\n",
    "\n",
    "    for anchors, positives, negatives, _ in progress_bar:\n",
    "        # 将数据移动到GPU上\n",
    "        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n",
    "\n",
    "        # 将数据展平\n",
    "        anchors, positives, negatives = anchors.view(anchors.size(0), -1), positives.view(positives.size(0),\n",
    "                                                                                          -1), negatives.view(\n",
    "            negatives.size(0), -1)\n",
    "\n",
    "        # 获取编码和解码的输出\n",
    "        anchor_encoded, anchor_decoded = model(anchors)\n",
    "        positive_encoded, _ = model(positives)\n",
    "        negative_encoded, _ = model(negatives)\n",
    "\n",
    "        # 计算重构损失和Triplet损失\n",
    "        r_loss = reconstruction_loss(anchor_decoded, anchors)\n",
    "        t_loss = triplet_loss(anchor_encoded, positive_encoded, negative_encoded)\n",
    "        loss = r_loss + t_loss\n",
    "        # loss = t_loss\n",
    "        # loss = r_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累计损失用于计算平均损失\n",
    "        total_loss += loss.item()\n",
    "        total_r_loss += r_loss.item()\n",
    "        total_t_loss += t_loss.item()\n",
    "\n",
    "        progress_bar.desc = f\"Epoch {epoch + 1}/{num_epochs}\"\n",
    "        progress_bar.set_postfix({\n",
    "            'total_loss': f'{total_loss / (progress_bar.n + 1):.4f}',\n",
    "            'recon_loss': f'{total_r_loss / (progress_bar.n + 1):.4f}',\n",
    "            'triplet_loss': f'{total_t_loss / (progress_bar.n + 1):.4f}'\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db75d1bef3ec515",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## run the test samples through the encoder\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_labels = []\n",
    "test_latent = []\n",
    "\n",
    "for inputs, _, _, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    inputs = inputs.view(-1, data_dim)\n",
    "    outputs = model.encoder(inputs)\n",
    "    test_latent.append(outputs.cpu().detach().numpy())\n",
    "    test_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "test_latent = np.concatenate(test_latent, axis=0)\n",
    "test_labels = np.concatenate(test_labels, axis=0)\n",
    "\n",
    "print(test_latent.shape, test_labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a7511c7394a7ae5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## calculate the centroid of each class\n",
    "\n",
    "centroids = np.zeros((11, latent_dim))\n",
    "for i in range(11):\n",
    "    idx = test_labels == i\n",
    "    centroids[i] = np.mean(test_latent[idx], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110bb36acad94271",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## plot the latent space\n",
    "%matplotlib tk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = np.unique(test_labels)\n",
    "label_map = {\n",
    "    0: 'call',\n",
    "    1: 'dislike',\n",
    "    2: 'fist',\n",
    "    3: 'like',\n",
    "    4: 'ok',\n",
    "    5: 'one',\n",
    "    6: 'palm',\n",
    "    7: 'peace',\n",
    "    8: 'rock',\n",
    "    9: 'three',\n",
    "    10: 'three2',\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Latent Space')\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    idx = test_labels == label\n",
    "\n",
    "    ax.scatter(test_latent[idx, 0],\n",
    "               test_latent[idx, 1],\n",
    "               c=np.array(matplotlib.colormaps['tab20'].colors[i]).reshape(1, -1),\n",
    "               label=f\"{label} {label_map[label]}\",\n",
    "               alpha=0.5)\n",
    "\n",
    "    ax.scatter(centroids[i, 0], centroids[i, 1], c='black', marker='x', s=100)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cf440b207c81971",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize and fit KMeans\n",
    "kmeans = KMeans(n_clusters=11, random_state=0).fit(test_latent)\n",
    "\n",
    "# Predict the cluster IDs for each data point\n",
    "cluster_ids = kmeans.predict(test_latent)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ea418b82bf7fd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Colors from a colormap\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Latent Space with K-Means Clustering')\n",
    "\n",
    "for cluster in range(11):\n",
    "    idx = cluster_ids == cluster\n",
    "    ax.scatter(test_latent[idx, 0],\n",
    "               test_latent[idx, 1],\n",
    "               c=np.array(matplotlib.colormaps['tab20'].colors[cluster]).reshape(1, -1),\n",
    "               label=f\"Cluster {cluster}\",\n",
    "               alpha=0.5)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f149412dc4934094",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Calculate clustering accuracy\n",
    "# We need to find the best match between cluster labels and true labels\n",
    "def clustering_accuracy(true_labels, cluster_labels):\n",
    "    # Confusion matrix between true labels and cluster labels\n",
    "    matrix = confusion_matrix(true_labels, cluster_labels)\n",
    "    # Summing the highest values in each column of the confusion matrix\n",
    "    max_matches = np.sum(np.max(matrix, axis=0))\n",
    "    accuracy = max_matches / len(true_labels)\n",
    "    return matrix, accuracy\n",
    "\n",
    "\n",
    "# Calculate and print the clustering accuracy\n",
    "matrix, accuracy = clustering_accuracy(test_labels, cluster_ids)\n",
    "print(f\"Clustering Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a05c59cc321b6f2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ## save model\n",
    "# torch.save(model.state_dict(), './model/triplet_autoencoder_9714.pth')\n",
    "# \n",
    "# ## save the centroids\n",
    "# np.save('./model/triplet_autoencoder_9714_centroids.npy', centroids)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb074c0ac6fd6da",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c48bf699f9353c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
